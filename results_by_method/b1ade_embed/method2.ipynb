{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9258033-5608-477d-b53e-dccab475ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import nltk\n",
    "import dspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import colorcet as cc\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb36b951-8ac4-40e2-aaae-fdeb7805a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_string(category, dictionary, n=None):\n",
    "    string = f\"{category}: \"\n",
    "\n",
    "    if category in dictionary:\n",
    "        category_data = dictionary[category]\n",
    "        if isinstance(category_data, list):\n",
    "            if category == 'characters' and n is not None:\n",
    "                category_data = category_data[:n]\n",
    "            for item in category_data:\n",
    "                for k, v in item.items():\n",
    "                    if k == 'name':\n",
    "                        continue\n",
    "                    if isinstance(v, list):\n",
    "                        v = ', '.join(v)\n",
    "                    string += f\"{k}: {v} \"\n",
    "        elif isinstance(category_data, dict):\n",
    "            for k, v in category_data.items():\n",
    "                if k == 'name':\n",
    "                    continue\n",
    "                if isinstance(v, list):\n",
    "                    v = ', '.join(v)\n",
    "                string += f\"{k}: {v} \"\n",
    "        else:\n",
    "            string += f\"{category}: {category_data} \"\n",
    "\n",
    "    return string.strip()\n",
    "\n",
    "def transform_dict_to_strings(data, n=None):\n",
    "    result = {}\n",
    "\n",
    "    for category in data:\n",
    "        result[category] = create_category_string(category, data, n=n)\n",
    "\n",
    "    return result\n",
    "\n",
    "def transform_dict_to_single_string(data, n=None):\n",
    "    parts = []\n",
    "\n",
    "    for category in data:\n",
    "        parts.append(create_category_string(category, data, n=n))\n",
    "\n",
    "    return \" \".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f23b24-d0f2-4689-98a1-1dcb2d59f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterAnonymizer:\n",
    "    def __init__(self, batch_size=16):\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=0\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def normalize_name(self, name):\n",
    "        name = name.lower()\n",
    "        name = re.sub(r\"[^a-z ]\", \"\", name)\n",
    "        return name.strip()\n",
    "\n",
    "    def extract_names_batch(self, texts):\n",
    "        ner_results_batch = self.ner_pipeline(texts)\n",
    "        all_names = []\n",
    "        for ner_results in ner_results_batch:\n",
    "            names = [ent['word'] for ent in ner_results if ent['entity_group'] == 'PER']\n",
    "            normalized = list(set(self.normalize_name(name) for name in names))\n",
    "            all_names.append(normalized)\n",
    "        return all_names\n",
    "\n",
    "    def cluster_names(self, names):\n",
    "        clusters = defaultdict(list)\n",
    "        used = set()\n",
    "\n",
    "        for name in names:\n",
    "            if not name.strip():\n",
    "                continue\n",
    "            if name in used:\n",
    "                continue\n",
    "            parts = name.split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            key = parts[0]\n",
    "            for other in names:\n",
    "                if key in other and other not in used:\n",
    "                    clusters[key].append(other)\n",
    "                    used.add(other)\n",
    "        return clusters\n",
    "\n",
    "    def generate_name_map(self, clusters):\n",
    "        name_map = {}\n",
    "        for i, (key, variants) in enumerate(clusters.items(), start=1):\n",
    "            tag = f\"Character{i}\"\n",
    "            for name in variants:\n",
    "                name_map[name] = tag\n",
    "        return name_map\n",
    "\n",
    "    def replace_names(self, text, name_map):\n",
    "        for original in sorted(name_map.keys(), key=len, reverse=True):\n",
    "            pattern = re.compile(rf'\\b{re.escape(original)}\\b', re.IGNORECASE)\n",
    "            text = pattern.sub(name_map[original], text)\n",
    "        return text\n",
    "\n",
    "    def anonymize_batch(self, texts):\n",
    "        anonymized_texts = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Anonymizing\"):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            all_names = self.extract_names_batch(batch)\n",
    "\n",
    "            for text, names in zip(batch, all_names):\n",
    "                clusters = self.cluster_names(names)\n",
    "                name_map = self.generate_name_map(clusters)\n",
    "                anonymized = self.replace_names(text, name_map)\n",
    "                anonymized_texts.append(anonymized)\n",
    "\n",
    "        return anonymized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa6ba2e-b5a7-4a65-895e-776810c12daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings, normalize_vectors=True):\n",
    "    if normalize_vectors:\n",
    "        return normalize(embeddings, axis=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_dense_similarity(query_vector, candidate_vectors):\n",
    "    if query_vector.ndim == 1:\n",
    "        query_vector = query_vector.reshape(1, -1)\n",
    "    return np.dot(candidate_vectors, query_vector.T).flatten()\n",
    "\n",
    "def get_relevance_scores(i, labels, index):\n",
    "    matching_label = labels[i]\n",
    "\n",
    "    query_vector = index[i].reshape(1, -1)\n",
    "    similarities = get_dense_similarity(query_vector, index).flatten()\n",
    "\n",
    "    relevance_scores = np.array([1 if x == matching_label else 0 for x in labels])\n",
    "    similarity_scores = similarities\n",
    "\n",
    "    mask = np.arange(len(labels)) != i\n",
    "\n",
    "    relevance_scores = relevance_scores[mask]\n",
    "    similarity_scores = similarity_scores[mask]\n",
    "\n",
    "    return relevance_scores.tolist(), similarity_scores.tolist()\n",
    "\n",
    "\n",
    "def calculate_mean_ndcg_score(labels, embeddings):\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    index = build_index(embeddings)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        relevance_scores, similarity_scores = get_relevance_scores(i, labels, index)\n",
    "        relevance_scores = np.array(relevance_scores)\n",
    "        similarity_scores = np.array(similarity_scores)\n",
    "        #k = relevance_scores.sum()\n",
    "        k = None\n",
    "        scores.append(ndcg_score([relevance_scores], [similarity_scores], k=k))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def calculate_mean_recall(labels, embeddings):\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    index = build_index(embeddings)\n",
    "\n",
    "    recalls = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        relevance_scores, similarity_scores = get_relevance_scores(i, labels, index)\n",
    "\n",
    "        relevance_scores = np.array(relevance_scores)\n",
    "        similarity_scores = np.array(similarity_scores)\n",
    "\n",
    "        k = relevance_scores.sum()\n",
    "        if k == 0:\n",
    "            continue\n",
    "\n",
    "        sorted_indices = np.argsort(-similarity_scores)\n",
    "\n",
    "        top_k_indices = sorted_indices[:k]\n",
    "\n",
    "        top_k_relevance = relevance_scores[top_k_indices]\n",
    "\n",
    "        recall = top_k_relevance.sum() / k\n",
    "        recalls.append(recall)\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(recalls)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb3d366-cb3e-473e-843b-02de1ebc496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(arr):\n",
    "    arr = np.array(arr)\n",
    "    min_val = arr.min()\n",
    "    max_val = arr.max()\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def get_dense_similarity(query_vec, matrix):\n",
    "    query_vec = np.array(query_vec).reshape(1, -1)\n",
    "    return cosine_similarity(query_vec, matrix)[0]\n",
    "\n",
    "\n",
    "\n",
    "#def transform_dict_to_strings(d):\n",
    "    #return {k: ' '.join(str(vv) for vv in v) if isinstance(v, list) else str(v) for k, v in d.items()}\n",
    "\n",
    "\n",
    "def get_similarity_scores_from_embeddings(df, index, keys):\n",
    "    keys.append(\"overall\")\n",
    "    similarities_by_key = {}\n",
    "\n",
    "    for key in keys:\n",
    "        embeddings = np.stack(df[f'embedding_{key}'].values)\n",
    "        target_embedding = embeddings[index]\n",
    "        similarities = get_dense_similarity(target_embedding, embeddings)\n",
    "        similarities_by_key[key] = min_max_scale(similarities)\n",
    "\n",
    "    all_similarities = np.zeros_like(next(iter(similarities_by_key.values())))\n",
    "\n",
    "    return similarities_by_key\n",
    "\n",
    "\n",
    "def make_spider(df, plot_idx, title, color, total_plots, overall_score, plots_per_row=5):\n",
    "    categories = list(df.columns)[1:]\n",
    "    if 'overall' in categories:\n",
    "        categories.remove('overall')\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    row_idx = plot_idx // plots_per_row\n",
    "    col_idx = plot_idx % plots_per_row\n",
    "    n_rows = (total_plots + plots_per_row - 1) // plots_per_row\n",
    "    n_cols = min(plots_per_row, total_plots)\n",
    "\n",
    "    ax = plt.subplot(n_rows, n_cols, plot_idx + 1, polar=True)\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    plt.xticks(angles[:-1], categories, color='black', size=10)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1], [\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    values = df.loc[plot_idx, categories].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, color=color, alpha=0.4)\n",
    "    wrapped_title = \"\\n\".join(title[i:i + 20] for i in range(0, len(title), 20))\n",
    "    plt.title(wrapped_title, size=18, color=color, y=1.08)\n",
    "\n",
    "    ax.text(0.5, 0.5, f\"{overall_score:.2f}\", transform=ax.transAxes,\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            fontsize=24, fontweight='bold', color=color)\n",
    "\n",
    "\n",
    "def visualize_similarities(df, model, target_index, top_n=4, plots_per_row=5, ascending=False, exclude_self=False, exclude_keys=None):\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"embedding_\")]\n",
    "\n",
    "    if exclude_keys is None:\n",
    "        exclude_keys = []\n",
    "\n",
    "    if not embedding_cols:\n",
    "        df, keys = embed_elements(df, model)\n",
    "    else:\n",
    "        keys = [col.replace(\"embedding_\", \"\") for col in embedding_cols \n",
    "                if col.replace(\"embedding_\", \"\") not in exclude_keys and col != \"embedding_overall\"]\n",
    "\n",
    "    similarities_by_key = get_similarity_scores_from_embeddings(df, index=target_index, keys=keys)\n",
    "\n",
    "    overall_similarities = similarities_by_key[\"overall\"]\n",
    "\n",
    "    sorted_indices = np.argsort(overall_similarities)\n",
    "    if not ascending:\n",
    "        sorted_indices = sorted_indices[::-1]\n",
    "\n",
    "    title_to_exclude = df.iloc[target_index][\"title\"] if exclude_self else None\n",
    "\n",
    "    filtered_indices = []\n",
    "    for i in sorted_indices:\n",
    "        if exclude_self and df.iloc[i][\"title\"] == title_to_exclude:\n",
    "            continue\n",
    "        if not exclude_self and i == target_index:\n",
    "            continue \n",
    "        filtered_indices.append(i)\n",
    "        if len(filtered_indices) == top_n:\n",
    "            break\n",
    "\n",
    "    rows = []\n",
    "    for i in filtered_indices:\n",
    "        title = f\"{df.iloc[i]['title']} ({df.iloc[i]['language']})\"\n",
    "        data_row = {'group': title}\n",
    "        for key in keys:\n",
    "            data_row[key] = similarities_by_key[key][i]\n",
    "        data_row['overall'] = overall_similarities[i]\n",
    "        rows.append(data_row)\n",
    "\n",
    "    query_title = f\"{df.iloc[target_index]['title']} ({df.iloc[target_index]['language']})\"\n",
    "    query_row = {'group': f\"Query: {query_title}\"}\n",
    "    for key in keys:\n",
    "        query_row[key] = similarities_by_key[key][target_index]\n",
    "    query_row['overall'] = overall_similarities[target_index]\n",
    "    rows.insert(0, query_row)\n",
    "\n",
    "    plot_data = pd.DataFrame(rows)\n",
    "\n",
    "    my_dpi = 96\n",
    "    width_per_plot = 500\n",
    "    height_per_plot = 500\n",
    "    total_plots = len(plot_data.index)\n",
    "    n_rows = (total_plots + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "    fig_width = width_per_plot * min(plots_per_row, total_plots) / my_dpi\n",
    "    fig_height = height_per_plot * n_rows / my_dpi\n",
    "\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=my_dpi)\n",
    "    base_titles = plot_data['group'].apply(lambda x: x.replace('Query: ', '').split(' (')[0])\n",
    "    unique_titles = base_titles.unique()\n",
    "    title_to_color_idx = {title: i for i, title in enumerate(unique_titles)}\n",
    "\n",
    "    my_palette = cc.glasbey_dark  \n",
    "\n",
    "    for plot_idx in range(total_plots):\n",
    "        base_title = base_titles.iloc[plot_idx]\n",
    "        color_idx = title_to_color_idx[base_title]\n",
    "        color = my_palette[color_idx]  \n",
    "    \n",
    "        make_spider(df=plot_data, plot_idx=plot_idx, title=plot_data['group'][plot_idx],\n",
    "                    color=color, total_plots=total_plots,\n",
    "                    overall_score=plot_data.loc[plot_idx, 'overall'],\n",
    "                    plots_per_row=plots_per_row)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"similarity_radar_plots.png\", dpi=my_dpi, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a561ad-a25e-4c53-a372-b843386a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_elements(df, model):\n",
    "    transformed_data = []\n",
    "    key_set = set()\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        element = json.loads(row.extracted_elements)\n",
    "        #element['full_summary'] = row.unpacked_summary # add a full summary embedding\n",
    "        transformed = transform_dict_to_strings(element, n=5)\n",
    "        transformed_data.append(transformed)\n",
    "        key_set.update(transformed)\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "    embedding_arrays = {}\n",
    "    for key in key_set:\n",
    "        texts = transformed_df[key].tolist()\n",
    "        embeddings = model.encode(texts, show_progress_bar=False)\n",
    "        normalized_embeddings = normalize(np.array(embeddings), norm='l2')\n",
    "        df[f'embedding_{key}'] = list(normalized_embeddings)\n",
    "        embedding_arrays[key] = normalized_embeddings\n",
    "\n",
    "    all_embeddings = np.stack([embedding_arrays[key] for key in key_set], axis=1)\n",
    "    mean_embeddings = np.mean(all_embeddings, axis=1)\n",
    "    df[\"embedding_overall\"] = list(normalize(mean_embeddings, norm='l2'))\n",
    "\n",
    "    return df, list(key_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903f08ea-bb56-492c-bc33-41f886a8fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=64, print_weights = False):\n",
    "        super().__init__()\n",
    "        self.weights = []\n",
    "        self.print_weights = print_weights\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        scores = self.attention(embeddings)  \n",
    "        weights = torch.softmax(scores, dim=0) \n",
    "        if self.print_weights:\n",
    "            self.weights.append(weights)\n",
    "            print(torch.stack(self.weights).mean(dim=0))\n",
    "        return (weights * embeddings).sum(dim=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cc4eb3-056e-4bd0-a873-9b0e378dc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticWeightedCombiner(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(num_embeddings)) \n",
    "\n",
    "    def forward(self, embeddings): \n",
    "        weights = torch.softmax(self.weights, dim=0)  \n",
    "        return (weights.unsqueeze(-1) * embeddings).sum(dim=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f12d3b-c866-4945-bc2d-36099831be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, embedding_cols, labels):\n",
    "        self.embedding_data = df[embedding_cols].values\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedding_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = [torch.tensor(e, dtype=torch.float32) for e in self.embedding_data[idx]]\n",
    "        label = self.labels[idx]\n",
    "        return torch.stack(embeddings), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbd6bed-337f-48a9-8921-f4202f646d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention(df, embedding_cols, labels, eval_df, eval_labels,\n",
    "                    epochs=10, batch_size=32, learning_rate=1e-3, margin=1.0):\n",
    "    \n",
    "    attention_model = SimpleAttention(len(df[embedding_cols[0]][0]))\n",
    "    #attention_model = StaticWeightedCombiner(num_embeddings = 6)\n",
    "    optimizer = torch.optim.Adam(attention_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    dataset = EmbeddingDataset(df, embedding_cols, encoded_labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        attention_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            embeddings_batch, labels_batch = batch\n",
    "            anchor_embs, positive_embs, negative_embs = [], [], []\n",
    "\n",
    "            for i in range(len(labels_batch)):\n",
    "                anchor = embeddings_batch[i]\n",
    "                label = labels_batch[i]\n",
    "\n",
    "                pos_indices = (labels_batch == label).nonzero(as_tuple=True)[0]\n",
    "                neg_indices = (labels_batch != label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if len(pos_indices) <= 1 or len(neg_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                while pos_idx == i:\n",
    "                    pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,)).item()]\n",
    "\n",
    "                anchor_embs.append(attention_model(anchor))\n",
    "                positive_embs.append(attention_model(embeddings_batch[pos_idx]))\n",
    "                negative_embs.append(attention_model(embeddings_batch[neg_idx]))\n",
    "\n",
    "            if anchor_embs:\n",
    "                anchor_embs = torch.stack(anchor_embs)\n",
    "                positive_embs = torch.stack(positive_embs)\n",
    "                negative_embs = torch.stack(negative_embs)\n",
    "\n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        avg_eval_loss = evaluate_attention(attention_model, eval_df, embedding_cols, eval_labels,\n",
    "                                           batch_size=batch_size, margin=margin)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.4f} | Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "    return attention_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4513a6-77de-44f8-8fb9-0d7d26b988fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attention(model, df, embedding_cols, labels, batch_size=32, margin=1.0):\n",
    "    model.eval()\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    dataset = EmbeddingDataset(df, embedding_cols, encoded_labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            embeddings_batch, labels_batch = batch\n",
    "            anchor_embs, positive_embs, negative_embs = [], [], []\n",
    "\n",
    "            for i in range(len(labels_batch)):\n",
    "                anchor = embeddings_batch[i]\n",
    "                label = labels_batch[i]\n",
    "\n",
    "                pos_indices = (labels_batch == label).nonzero(as_tuple=True)[0]\n",
    "                neg_indices = (labels_batch != label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if len(pos_indices) <= 1 or len(neg_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                while pos_idx == i:\n",
    "                    pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,)).item()]\n",
    "\n",
    "                anchor_embs.append(model(anchor))\n",
    "                positive_embs.append(model(embeddings_batch[pos_idx]))\n",
    "                negative_embs.append(model(embeddings_batch[neg_idx]))\n",
    "\n",
    "            if anchor_embs:\n",
    "                anchor_embs = torch.stack(anchor_embs)\n",
    "                positive_embs = torch.stack(positive_embs)\n",
    "                negative_embs = torch.stack(negative_embs)\n",
    "\n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "    return total_loss / max(count, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "331022e6-8a53-4e00-97fb-4bf2f00966b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attention(df, embedding_cols, model):\n",
    "    df[\"embedding_overall\"] = df[embedding_cols].apply(\n",
    "        lambda row: model(\n",
    "            torch.stack([torch.tensor(x, dtype=torch.float32) for x in row])\n",
    "        ).detach().numpy(),\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6e0d633-d285-4391-accb-ed86a2d254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name w601sxs/b1ade-embed. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentenceTransformer(\"w601sxs/b1ade-embed\", trust_remote_code=True, device=device)\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "df = pd.read_excel(\"../../data/tell_me_again_df_with_elements.xlsx\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92579d-8d7a-491f-8525-b66199bf52d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477655b8-74ae-4944-a446-b88f681ab226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': [{'name': 'James Bond',\n",
       "   'role': 'protagonist',\n",
       "   'backstory': \"A special agent for England's secret service, known for his skill and composure.\",\n",
       "   'weaknesses': [],\n",
       "   'strengths': ['skill', 'composure', 'resourcefulness'],\n",
       "   'motivations': ['completing his mission',\n",
       "    \"discovering Le Chiffre's plans\"]},\n",
       "  {'name': 'Le Chiffre',\n",
       "   'role': 'antagonist',\n",
       "   'backstory': 'Treasurer of the Soviet counterintelligence agency SODA.',\n",
       "   'weaknesses': ['greed', 'reliance on cheating'],\n",
       "   'strengths': ['financial manipulation', 'ruthlessness'],\n",
       "   'motivations': ['recovering lost funds',\n",
       "    'sabotaging Western intelligence']},\n",
       "  {'name': 'Vesper Lynd',\n",
       "   'role': 'deuteragonist',\n",
       "   'backstory': \"Personal assistant to the head of the Soviet Union's S Section, secretly a double agent.\",\n",
       "   'weaknesses': ['blackmail', 'emotional vulnerability'],\n",
       "   'strengths': ['intelligence', 'deception'],\n",
       "   'motivations': ['protecting her boyfriend', 'escaping SMERSH']},\n",
       "  {'name': 'Felix Leiter',\n",
       "   'role': 'confidant',\n",
       "   'backstory': 'CIA agent.',\n",
       "   'weaknesses': [],\n",
       "   'strengths': ['assistance', 'resourcefulness'],\n",
       "   'motivations': ['helping Bond']},\n",
       "  {'name': 'Gettler',\n",
       "   'role': 'tertiary character',\n",
       "   'backstory': 'Agent of SMERSH.',\n",
       "   'weaknesses': [],\n",
       "   'strengths': ['surveillance'],\n",
       "   'motivations': ['monitoring Bond and Lynd']}],\n",
       " 'setting': {'time_periods': ['1990s'],\n",
       "  'locations': ['England', 'northern France', 'Royale-Les-Eaux Casino'],\n",
       "  'cultural_context': ['Cold War espionage',\n",
       "   'international intelligence agencies']},\n",
       " 'plot': {'conflict': 'person vs. person',\n",
       "  'basic_plot': 'overcoming the Monster',\n",
       "  'story_exposition_summary': 'James Bond is assigned to bankrupt Le Chiffre, a Soviet agent, at a casino. He is partnered with Vesper Lynd, who is secretly a double agent. The story begins with the setup of the mission and the introduction of key players.',\n",
       "  'story_rising_action_summary': 'Bond and Le Chiffre engage in a high-stakes baccarat game. Bond initially loses but receives assistance from Felix Leiter. Le Chiffre attempts to kill Bond, but fails. Bond ultimately wins the game, taking a large sum of money from Le Chiffre.',\n",
       "  'story_climax_summary': 'Le Chiffre kidnaps Bond and Vesper, subjecting them to torture. A SMERSH agent murders Le Chiffre as punishment for his failure.',\n",
       "  'story_falling_action_summary': 'Bond and Vesper recover, and they develop a romantic relationship. However, their happiness is threatened by the presence of Gettler, a SMERSH agent.',\n",
       "  'story_resolution_summary': 'Vesper reveals her double agent status and commits suicide to protect Bond from SMERSH. Bond informs his service of her betrayal.'},\n",
       " 'theme': {'themes': ['betrayal', 'love', 'espionage', 'redemption'],\n",
       "  'morals': ['trust is fragile',\n",
       "   'appearances can be deceiving',\n",
       "   'sacrifice for the greater good']},\n",
       " 'other': {'main_genres': ['thriller', 'action', 'romance'],\n",
       "  'genre_keywords': ['espionage', 'suspense', 'double agent', 'torture']}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(df.iloc[52].extracted_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c7fcc4-12f3-4e97-b054-1a6b542fb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb615f5c0614d7fa2888f1e5fa65b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 344 ms\n",
      "Wall time: 256 ms\n"
     ]
    }
   ],
   "source": [
    "embs = model.encode(strings, batch_size=15, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0902d70-7ae0-4d55-a5ee-d894205c6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding_overall'] = list(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f59e29f-ad05-420e-99ed-2dc528ae4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Fold 1 NDCG: 0.9831786123478419\n",
      "Fold 1 Recall: 0.9578392621870883\n",
      "\n",
      "--- Fold 2 ---\n",
      "Fold 2 NDCG: 0.9861250106984674\n",
      "Fold 2 Recall: 0.963987703118138\n",
      "\n",
      "--- Fold 3 ---\n",
      "Fold 3 NDCG: 0.9902982774200954\n",
      "Fold 3 Recall: 0.97556142668428\n",
      "\n",
      "--- Fold 4 ---\n",
      "Fold 4 NDCG: 0.9812734650380047\n",
      "Fold 4 Recall: 0.9535446939674153\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 NDCG: 0.9885917662944129\n",
      "Fold 5 Recall: 0.9725153913808267\n",
      "\n",
      "=== Cross-Validation Summary ===\n",
      "Average NDCG: 0.9859 ± 0.0033\n",
      "Average Recall: 0.9647 ± 0.0084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "\n",
    "set_seed(42)\n",
    "embedding_cols = [col for col in df.columns if col.startswith(\"embedding_\")]\n",
    "\n",
    "if not embedding_cols:\n",
    "    df, key_set = embed_elements(df, model)\n",
    "\n",
    "labels = df['label'].tolist()\n",
    "groups = df['label']\n",
    "n_splits = 5\n",
    "\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "ndcg_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, labels, groups=groups)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "    \n",
    "    df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    embedding_cols = [\"embedding_characters\", \"embedding_setting\", \"embedding_plot\", \"embedding_theme\", \"embedding_other\"]#\n",
    "\n",
    "    #attention_model = train_attention(\n",
    "    #    df_train, embedding_cols, df_train.label.tolist(),\n",
    "    #   df_val, df_val.label.tolist(),\n",
    "    #    #\n",
    "    #   epochs=100, batch_size=128, learning_rate=0.0001, margin=0.1\n",
    "    #)\n",
    "\n",
    "    #df_val = apply_attention(df_val, embedding_cols, attention_model)\n",
    "\n",
    "    ndcg = calculate_mean_ndcg_score(\n",
    "        labels=df_val.label.tolist(),\n",
    "        embeddings=df_val.embedding_overall.tolist()\n",
    "    )\n",
    "    recall = calculate_mean_recall(\n",
    "        labels=df_val.label.tolist(),\n",
    "        embeddings=df_val.embedding_overall.tolist()\n",
    "    )\n",
    "\n",
    "    ndcg_scores.append(ndcg)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"Fold {fold+1} NDCG: {ndcg}\\nFold {fold+1} Recall: {recall}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Cross-Validation Summary ===\")\n",
    "print(f\"Average NDCG: {np.mean(ndcg_scores):.4f} ± {np.std(ndcg_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f89a136-9ee1-4773-ad5a-5b3243c5364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>unpacked_summary</th>\n",
       "      <th>unpacked_summary_sents</th>\n",
       "      <th>label</th>\n",
       "      <th>property_count</th>\n",
       "      <th>extracted_elements</th>\n",
       "      <th>id_count</th>\n",
       "      <th>labels</th>\n",
       "      <th>embedding_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>Black Box</td>\n",
       "      <td>de</td>\n",
       "      <td>In a car accident, young mother Rachel is kill...</td>\n",
       "      <td>['In a car accident, young mother Rachel is ki...</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Nolan\",\"role\":\"protago...</td>\n",
       "      <td>2</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>[0.9594285, -0.65563273, 0.31010288, 0.0407096...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>Black Box</td>\n",
       "      <td>it</td>\n",
       "      <td>Nolan is a man who suffers from severe amnesia...</td>\n",
       "      <td>['Nolan is a man who suffers from severe amnes...</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Nolan/Thomas\",\"role\":\"...</td>\n",
       "      <td>2</td>\n",
       "      <td>Q100265988</td>\n",
       "      <td>[0.8943757, -0.4857412, 0.10760754, -0.1563213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>Decision to Leave</td>\n",
       "      <td>de</td>\n",
       "      <td>The experienced Commissioner Jang Hae-joon is ...</td>\n",
       "      <td>['The experienced Commissioner Jang Hae-joon i...</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Jang Hae-joon\",\"role\":...</td>\n",
       "      <td>3</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>[0.6579062, -1.1487026, -0.057919126, -0.21468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>Decision to Leave</td>\n",
       "      <td>fr</td>\n",
       "      <td>Hae-joon, a police officer working in Busan, i...</td>\n",
       "      <td>['Hae-joon, a police officer working in Busan,...</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Hae-joon\",\"role\":\"prot...</td>\n",
       "      <td>3</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>[0.67277646, -1.1121638, -0.015551006, -0.1013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>Decision to Leave</td>\n",
       "      <td>it</td>\n",
       "      <td>Detective Jang Hae-jun, who suffers from insom...</td>\n",
       "      <td>['Detective Jang Hae-jun, who suffers from ins...</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Detective Jang Hae-jun...</td>\n",
       "      <td>3</td>\n",
       "      <td>Q100889636</td>\n",
       "      <td>[0.48955315, -0.69824857, -0.005556168, -0.482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>83185</td>\n",
       "      <td>83185</td>\n",
       "      <td>Q99671372</td>\n",
       "      <td>Borat Subsequent Moviefilm</td>\n",
       "      <td>it</td>\n",
       "      <td>After fourteen years of forced labor in a gula...</td>\n",
       "      <td>[\"After fourteen years of forced labor in a gu...</td>\n",
       "      <td>Q99671372</td>\n",
       "      <td>3</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Borat Sagdiyev\",\"role\"...</td>\n",
       "      <td>3</td>\n",
       "      <td>Q99671372</td>\n",
       "      <td>[1.1949809, -1.2937195, -0.53089887, -0.037736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>83187</td>\n",
       "      <td>83187</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>Old</td>\n",
       "      <td>de</td>\n",
       "      <td>Guy and Prisca Cappa are going on vacation wit...</td>\n",
       "      <td>['Guy and Prisca Cappa are going on vacation w...</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...</td>\n",
       "      <td>4</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>[0.91239846, -0.7323379, 0.82740223, 0.0077040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>83188</td>\n",
       "      <td>83188</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>Old</td>\n",
       "      <td>fr</td>\n",
       "      <td>A family on vacation discovers that the seclud...</td>\n",
       "      <td>['A family on vacation discovers that the secl...</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...</td>\n",
       "      <td>4</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>[0.6351481, -0.8999526, 0.5772989, 0.17468996,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>83189</td>\n",
       "      <td>83189</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>Old</td>\n",
       "      <td>it</td>\n",
       "      <td>Guy and Prisca Cappa, husband and wife, travel...</td>\n",
       "      <td>['Guy and Prisca Cappa, husband and wife, trav...</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...</td>\n",
       "      <td>4</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>[1.0064925, -0.7724705, 0.94527936, 0.06869117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>83190</td>\n",
       "      <td>83190</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>Old</td>\n",
       "      <td>es</td>\n",
       "      <td>Married couple Guy and Prisca travel to a reso...</td>\n",
       "      <td>['Married couple Guy and Prisca travel to a re...</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>4</td>\n",
       "      <td>{\"characters\":[{\"name\":\"Guy\",\"role\":\"protagoni...</td>\n",
       "      <td>4</td>\n",
       "      <td>Q99688807</td>\n",
       "      <td>[0.9120029, -0.8031671, 0.5871778, -0.21069653...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3790 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1 wikidata_id                       title  \\\n",
       "0             48            48  Q100265988                   Black Box   \n",
       "1             49            49  Q100265988                   Black Box   \n",
       "2            128           128  Q100889636           Decision to Leave   \n",
       "3            129           129  Q100889636           Decision to Leave   \n",
       "4            130           130  Q100889636           Decision to Leave   \n",
       "...          ...           ...         ...                         ...   \n",
       "3785       83185         83185   Q99671372  Borat Subsequent Moviefilm   \n",
       "3786       83187         83187   Q99688807                         Old   \n",
       "3787       83188         83188   Q99688807                         Old   \n",
       "3788       83189         83189   Q99688807                         Old   \n",
       "3789       83190         83190   Q99688807                         Old   \n",
       "\n",
       "     language                                   unpacked_summary  \\\n",
       "0          de  In a car accident, young mother Rachel is kill...   \n",
       "1          it  Nolan is a man who suffers from severe amnesia...   \n",
       "2          de  The experienced Commissioner Jang Hae-joon is ...   \n",
       "3          fr  Hae-joon, a police officer working in Busan, i...   \n",
       "4          it  Detective Jang Hae-jun, who suffers from insom...   \n",
       "...       ...                                                ...   \n",
       "3785       it  After fourteen years of forced labor in a gula...   \n",
       "3786       de  Guy and Prisca Cappa are going on vacation wit...   \n",
       "3787       fr  A family on vacation discovers that the seclud...   \n",
       "3788       it  Guy and Prisca Cappa, husband and wife, travel...   \n",
       "3789       es  Married couple Guy and Prisca travel to a reso...   \n",
       "\n",
       "                                 unpacked_summary_sents       label  \\\n",
       "0     ['In a car accident, young mother Rachel is ki...  Q100265988   \n",
       "1     ['Nolan is a man who suffers from severe amnes...  Q100265988   \n",
       "2     ['The experienced Commissioner Jang Hae-joon i...  Q100889636   \n",
       "3     ['Hae-joon, a police officer working in Busan,...  Q100889636   \n",
       "4     ['Detective Jang Hae-jun, who suffers from ins...  Q100889636   \n",
       "...                                                 ...         ...   \n",
       "3785  [\"After fourteen years of forced labor in a gu...   Q99671372   \n",
       "3786  ['Guy and Prisca Cappa are going on vacation w...   Q99688807   \n",
       "3787  ['A family on vacation discovers that the secl...   Q99688807   \n",
       "3788  ['Guy and Prisca Cappa, husband and wife, trav...   Q99688807   \n",
       "3789  ['Married couple Guy and Prisca travel to a re...   Q99688807   \n",
       "\n",
       "      property_count                                 extracted_elements  \\\n",
       "0                  4  {\"characters\":[{\"name\":\"Nolan\",\"role\":\"protago...   \n",
       "1                  4  {\"characters\":[{\"name\":\"Nolan/Thomas\",\"role\":\"...   \n",
       "2                  3  {\"characters\":[{\"name\":\"Jang Hae-joon\",\"role\":...   \n",
       "3                  3  {\"characters\":[{\"name\":\"Hae-joon\",\"role\":\"prot...   \n",
       "4                  3  {\"characters\":[{\"name\":\"Detective Jang Hae-jun...   \n",
       "...              ...                                                ...   \n",
       "3785               3  {\"characters\":[{\"name\":\"Borat Sagdiyev\",\"role\"...   \n",
       "3786               4  {\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...   \n",
       "3787               4  {\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...   \n",
       "3788               4  {\"characters\":[{\"name\":\"Guy Cappa\",\"role\":\"pro...   \n",
       "3789               4  {\"characters\":[{\"name\":\"Guy\",\"role\":\"protagoni...   \n",
       "\n",
       "      id_count      labels                                  embedding_overall  \n",
       "0            2  Q100265988  [0.9594285, -0.65563273, 0.31010288, 0.0407096...  \n",
       "1            2  Q100265988  [0.8943757, -0.4857412, 0.10760754, -0.1563213...  \n",
       "2            3  Q100889636  [0.6579062, -1.1487026, -0.057919126, -0.21468...  \n",
       "3            3  Q100889636  [0.67277646, -1.1121638, -0.015551006, -0.1013...  \n",
       "4            3  Q100889636  [0.48955315, -0.69824857, -0.005556168, -0.482...  \n",
       "...        ...         ...                                                ...  \n",
       "3785         3   Q99671372  [1.1949809, -1.2937195, -0.53089887, -0.037736...  \n",
       "3786         4   Q99688807  [0.91239846, -0.7323379, 0.82740223, 0.0077040...  \n",
       "3787         4   Q99688807  [0.6351481, -0.8999526, 0.5772989, 0.17468996,...  \n",
       "3788         4   Q99688807  [1.0064925, -0.7724705, 0.94527936, 0.06869117...  \n",
       "3789         4   Q99688807  [0.9120029, -0.8031671, 0.5871778, -0.21069653...  \n",
       "\n",
       "[3790 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890d6496-6130-4e50-b12b-1fdb483a7295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.wikidata_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
