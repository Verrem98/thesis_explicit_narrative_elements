{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9258033-5608-477d-b53e-dccab475ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emiel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import nltk\n",
    "import dspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import matplotlib\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "import colorcet as cc\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb36b951-8ac4-40e2-aaae-fdeb7805a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_string(category, dictionary, n=None):\n",
    "    string = f\"{category}: \"\n",
    "\n",
    "    if category in dictionary:\n",
    "        category_data = dictionary[category]\n",
    "        if isinstance(category_data, list):\n",
    "            if category == 'characters' and n is not None:\n",
    "                category_data = category_data[:n]\n",
    "            for item in category_data:\n",
    "                for k, v in item.items():\n",
    "                    if k == 'name':\n",
    "                        continue\n",
    "                    if isinstance(v, list):\n",
    "                        v = ', '.join(v)\n",
    "                    string += f\"{k}: {v} \"\n",
    "        elif isinstance(category_data, dict):\n",
    "            for k, v in category_data.items():\n",
    "                if k == 'name':\n",
    "                    continue\n",
    "                if isinstance(v, list):\n",
    "                    v = ', '.join(v)\n",
    "                string += f\"{k}: {v} \"\n",
    "        else:\n",
    "            string += f\"{category}: {category_data} \"\n",
    "\n",
    "    return string.strip()\n",
    "\n",
    "def transform_dict_to_strings(data, n=None):\n",
    "    result = {}\n",
    "\n",
    "    for category in data:\n",
    "        result[category] = create_category_string(category, data, n=n)\n",
    "\n",
    "    return result\n",
    "\n",
    "def transform_dict_to_single_string(data, n=None):\n",
    "    parts = []\n",
    "\n",
    "    for category in data:\n",
    "        parts.append(create_category_string(category, data, n=n))\n",
    "\n",
    "    return \" \".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f23b24-d0f2-4689-98a1-1dcb2d59f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterAnonymizer:\n",
    "    def __init__(self, batch_size=16):\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=0\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def normalize_name(self, name):\n",
    "        name = name.lower()\n",
    "        name = re.sub(r\"[^a-z ]\", \"\", name)\n",
    "        return name.strip()\n",
    "\n",
    "    def extract_names_batch(self, texts):\n",
    "        ner_results_batch = self.ner_pipeline(texts)\n",
    "        all_names = []\n",
    "        for ner_results in ner_results_batch:\n",
    "            names = [ent['word'] for ent in ner_results if ent['entity_group'] == 'PER']\n",
    "            normalized = list(set(self.normalize_name(name) for name in names))\n",
    "            all_names.append(normalized)\n",
    "        return all_names\n",
    "\n",
    "    def cluster_names(self, names):\n",
    "        clusters = defaultdict(list)\n",
    "        used = set()\n",
    "\n",
    "        for name in names:\n",
    "            if not name.strip():\n",
    "                continue\n",
    "            if name in used:\n",
    "                continue\n",
    "            parts = name.split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            key = parts[0]\n",
    "            for other in names:\n",
    "                if key in other and other not in used:\n",
    "                    clusters[key].append(other)\n",
    "                    used.add(other)\n",
    "        return clusters\n",
    "\n",
    "    def generate_name_map(self, clusters):\n",
    "        name_map = {}\n",
    "        for i, (key, variants) in enumerate(clusters.items(), start=1):\n",
    "            tag = f\"Character{i}\"\n",
    "            for name in variants:\n",
    "                name_map[name] = tag\n",
    "        return name_map\n",
    "\n",
    "    def replace_names(self, text, name_map):\n",
    "        for original in sorted(name_map.keys(), key=len, reverse=True):\n",
    "            pattern = re.compile(rf'\\b{re.escape(original)}\\b', re.IGNORECASE)\n",
    "            text = pattern.sub(name_map[original], text)\n",
    "        return text\n",
    "\n",
    "    def anonymize_batch(self, texts):\n",
    "        anonymized_texts = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Anonymizing\"):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            all_names = self.extract_names_batch(batch)\n",
    "\n",
    "            for text, names in zip(batch, all_names):\n",
    "                clusters = self.cluster_names(names)\n",
    "                name_map = self.generate_name_map(clusters)\n",
    "                anonymized = self.replace_names(text, name_map)\n",
    "                anonymized_texts.append(anonymized)\n",
    "\n",
    "        return anonymized_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa6ba2e-b5a7-4a65-895e-776810c12daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings, normalize_vectors=True):\n",
    "    if normalize_vectors:\n",
    "        return normalize(embeddings, axis=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_dense_similarity(query_vector, candidate_vectors):\n",
    "    if query_vector.ndim == 1:\n",
    "        query_vector = query_vector.reshape(1, -1)\n",
    "    return np.dot(candidate_vectors, query_vector.T).flatten()\n",
    "\n",
    "def get_relevance_scores(i, labels, index):\n",
    "    matching_label = labels[i]\n",
    "\n",
    "    query_vector = index[i].reshape(1, -1)\n",
    "    similarities = get_dense_similarity(query_vector, index).flatten()\n",
    "\n",
    "    relevance_scores = np.array([1 if x == matching_label else 0 for x in labels])\n",
    "    similarity_scores = similarities\n",
    "\n",
    "    mask = np.arange(len(labels)) != i\n",
    "\n",
    "    relevance_scores = relevance_scores[mask]\n",
    "    similarity_scores = similarity_scores[mask]\n",
    "\n",
    "    return relevance_scores.tolist(), similarity_scores.tolist()\n",
    "\n",
    "\n",
    "def calculate_mean_ndcg_score(labels, embeddings):\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    index = build_index(embeddings)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        relevance_scores, similarity_scores = get_relevance_scores(i, labels, index)\n",
    "        relevance_scores = np.array(relevance_scores)\n",
    "        similarity_scores = np.array(similarity_scores)\n",
    "        #k = relevance_scores.sum()\n",
    "        k = None\n",
    "        scores.append(ndcg_score([relevance_scores], [similarity_scores], k=k))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def calculate_mean_recall(labels, embeddings):\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    index = build_index(embeddings)\n",
    "\n",
    "    recalls = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        relevance_scores, similarity_scores = get_relevance_scores(i, labels, index)\n",
    "\n",
    "        relevance_scores = np.array(relevance_scores)\n",
    "        similarity_scores = np.array(similarity_scores)\n",
    "\n",
    "        k = relevance_scores.sum()\n",
    "        if k == 0:\n",
    "            continue\n",
    "\n",
    "        sorted_indices = np.argsort(-similarity_scores)\n",
    "\n",
    "        top_k_indices = sorted_indices[:k]\n",
    "\n",
    "        top_k_relevance = relevance_scores[top_k_indices]\n",
    "\n",
    "        recall = top_k_relevance.sum() / k\n",
    "        recalls.append(recall)\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(recalls)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb3d366-cb3e-473e-843b-02de1ebc496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(arr):\n",
    "    arr = np.array(arr)\n",
    "    min_val = arr.min()\n",
    "    max_val = arr.max()\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def get_dense_similarity(query_vec, matrix):\n",
    "    query_vec = np.array(query_vec).reshape(1, -1)\n",
    "    return cosine_similarity(query_vec, matrix)[0]\n",
    "\n",
    "\n",
    "\n",
    "#def transform_dict_to_strings(d):\n",
    "    #return {k: ' '.join(str(vv) for vv in v) if isinstance(v, list) else str(v) for k, v in d.items()}\n",
    "\n",
    "\n",
    "def get_similarity_scores_from_embeddings(df, index, keys):\n",
    "    keys.append(\"overall\")\n",
    "    similarities_by_key = {}\n",
    "\n",
    "    for key in keys:\n",
    "        embeddings = np.stack(df[f'embedding_{key}'].values)\n",
    "        target_embedding = embeddings[index]\n",
    "        similarities = get_dense_similarity(target_embedding, embeddings)\n",
    "        similarities_by_key[key] = min_max_scale(similarities)\n",
    "\n",
    "    all_similarities = np.zeros_like(next(iter(similarities_by_key.values())))\n",
    "\n",
    "    return similarities_by_key\n",
    "\n",
    "\n",
    "def make_spider(df, plot_idx, title, color, total_plots, overall_score, plots_per_row=5):\n",
    "    categories = list(df.columns)[1:]\n",
    "    if 'overall' in categories:\n",
    "        categories.remove('overall')\n",
    "    N = len(categories)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    row_idx = plot_idx // plots_per_row\n",
    "    col_idx = plot_idx % plots_per_row\n",
    "    n_rows = (total_plots + plots_per_row - 1) // plots_per_row\n",
    "    n_cols = min(plots_per_row, total_plots)\n",
    "\n",
    "    ax = plt.subplot(n_rows, n_cols, plot_idx + 1, polar=True)\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    plt.xticks(angles[:-1], categories, color='black', size=10)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1], [\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    values = df.loc[plot_idx, categories].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, color=color, alpha=0.4)\n",
    "    wrapped_title = \"\\n\".join(title[i:i + 20] for i in range(0, len(title), 20))\n",
    "    plt.title(wrapped_title, size=18, color=color, y=1.08)\n",
    "\n",
    "    ax.text(0.5, 0.5, f\"{overall_score:.2f}\", transform=ax.transAxes,\n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            fontsize=24, fontweight='bold', color=color)\n",
    "\n",
    "\n",
    "def visualize_similarities(df, model, target_index, top_n=4, plots_per_row=5, ascending=False, exclude_self=False, exclude_keys=None):\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"embedding_\")]\n",
    "\n",
    "    if exclude_keys is None:\n",
    "        exclude_keys = []\n",
    "\n",
    "    if not embedding_cols:\n",
    "        df, keys = embed_elements(df, model)\n",
    "    else:\n",
    "        keys = [col.replace(\"embedding_\", \"\") for col in embedding_cols \n",
    "                if col.replace(\"embedding_\", \"\") not in exclude_keys and col != \"embedding_overall\"]\n",
    "\n",
    "    similarities_by_key = get_similarity_scores_from_embeddings(df, index=target_index, keys=keys)\n",
    "\n",
    "    overall_similarities = similarities_by_key[\"overall\"]\n",
    "\n",
    "    sorted_indices = np.argsort(overall_similarities)\n",
    "    if not ascending:\n",
    "        sorted_indices = sorted_indices[::-1]\n",
    "\n",
    "    title_to_exclude = df.iloc[target_index][\"title\"] if exclude_self else None\n",
    "\n",
    "    filtered_indices = []\n",
    "    for i in sorted_indices:\n",
    "        if exclude_self and df.iloc[i][\"title\"] == title_to_exclude:\n",
    "            continue\n",
    "        if not exclude_self and i == target_index:\n",
    "            continue \n",
    "        filtered_indices.append(i)\n",
    "        if len(filtered_indices) == top_n:\n",
    "            break\n",
    "\n",
    "    rows = []\n",
    "    for i in filtered_indices:\n",
    "        title = f\"{df.iloc[i]['title']} ({df.iloc[i]['language']})\"\n",
    "        data_row = {'group': title}\n",
    "        for key in keys:\n",
    "            data_row[key] = similarities_by_key[key][i]\n",
    "        data_row['overall'] = overall_similarities[i]\n",
    "        rows.append(data_row)\n",
    "\n",
    "    query_title = f\"{df.iloc[target_index]['title']} ({df.iloc[target_index]['language']})\"\n",
    "    query_row = {'group': f\"Query: {query_title}\"}\n",
    "    for key in keys:\n",
    "        query_row[key] = similarities_by_key[key][target_index]\n",
    "    query_row['overall'] = overall_similarities[target_index]\n",
    "    rows.insert(0, query_row)\n",
    "\n",
    "    plot_data = pd.DataFrame(rows)\n",
    "\n",
    "    my_dpi = 96\n",
    "    width_per_plot = 500\n",
    "    height_per_plot = 500\n",
    "    total_plots = len(plot_data.index)\n",
    "    n_rows = (total_plots + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "    fig_width = width_per_plot * min(plots_per_row, total_plots) / my_dpi\n",
    "    fig_height = height_per_plot * n_rows / my_dpi\n",
    "\n",
    "    plt.figure(figsize=(fig_width, fig_height), dpi=my_dpi)\n",
    "    base_titles = plot_data['group'].apply(lambda x: x.replace('Query: ', '').split(' (')[0])\n",
    "    unique_titles = base_titles.unique()\n",
    "    title_to_color_idx = {title: i for i, title in enumerate(unique_titles)}\n",
    "\n",
    "    my_palette = cc.glasbey_dark  \n",
    "\n",
    "    for plot_idx in range(total_plots):\n",
    "        base_title = base_titles.iloc[plot_idx]\n",
    "        color_idx = title_to_color_idx[base_title]\n",
    "        color = my_palette[color_idx]  \n",
    "    \n",
    "        make_spider(df=plot_data, plot_idx=plot_idx, title=plot_data['group'][plot_idx],\n",
    "                    color=color, total_plots=total_plots,\n",
    "                    overall_score=plot_data.loc[plot_idx, 'overall'],\n",
    "                    plots_per_row=plots_per_row)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"similarity_radar_plots.png\", dpi=my_dpi, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a561ad-a25e-4c53-a372-b843386a135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_elements(df, model):\n",
    "    transformed_data = []\n",
    "    key_set = set()\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        element = json.loads(row.extracted_elements)\n",
    "        #element['full_summary'] = row.unpacked_summary # add a full summary embedding\n",
    "        transformed = transform_dict_to_strings(element, n=5)\n",
    "        transformed_data.append(transformed)\n",
    "        key_set.update(transformed)\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed_data)\n",
    "\n",
    "    embedding_arrays = {}\n",
    "    for key in key_set:\n",
    "        texts = transformed_df[key].tolist()\n",
    "        embeddings = model.encode(texts, show_progress_bar=False)\n",
    "        normalized_embeddings = normalize(np.array(embeddings), norm='l2')\n",
    "        df[f'embedding_{key}'] = list(normalized_embeddings)\n",
    "        embedding_arrays[key] = normalized_embeddings\n",
    "\n",
    "    all_embeddings = np.stack([embedding_arrays[key] for key in key_set], axis=1)\n",
    "    mean_embeddings = np.mean(all_embeddings, axis=1)\n",
    "    df[\"embedding_overall\"] = list(normalize(mean_embeddings, norm='l2'))\n",
    "\n",
    "    return df, list(key_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903f08ea-bb56-492c-bc33-41f886a8fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=64, print_weights = False):\n",
    "        super().__init__()\n",
    "        self.weights = []\n",
    "        self.print_weights = print_weights\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        scores = self.attention(embeddings)  \n",
    "        weights = torch.softmax(scores, dim=0) \n",
    "        if self.print_weights:\n",
    "            self.weights.append(weights)\n",
    "            print(torch.stack(self.weights).mean(dim=0))\n",
    "        return (weights * embeddings).sum(dim=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cc4eb3-056e-4bd0-a873-9b0e378dc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticWeightedCombiner(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(num_embeddings)) \n",
    "\n",
    "    def forward(self, embeddings): \n",
    "        weights = torch.softmax(self.weights, dim=0)  \n",
    "        return (weights.unsqueeze(-1) * embeddings).sum(dim=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f12d3b-c866-4945-bc2d-36099831be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, embedding_cols, labels):\n",
    "        self.embedding_data = df[embedding_cols].values\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedding_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = [torch.tensor(e, dtype=torch.float32) for e in self.embedding_data[idx]]\n",
    "        label = self.labels[idx]\n",
    "        return torch.stack(embeddings), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbd6bed-337f-48a9-8921-f4202f646d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention(df, embedding_cols, labels, eval_df, eval_labels,\n",
    "                    epochs=10, batch_size=32, learning_rate=1e-3, margin=1.0):\n",
    "    \n",
    "    attention_model = SimpleAttention(len(df[embedding_cols[0]][0]))\n",
    "    #attention_model = StaticWeightedCombiner(num_embeddings = 6)\n",
    "    optimizer = torch.optim.Adam(attention_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    dataset = EmbeddingDataset(df, embedding_cols, encoded_labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        attention_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            embeddings_batch, labels_batch = batch\n",
    "            anchor_embs, positive_embs, negative_embs = [], [], []\n",
    "\n",
    "            for i in range(len(labels_batch)):\n",
    "                anchor = embeddings_batch[i]\n",
    "                label = labels_batch[i]\n",
    "\n",
    "                pos_indices = (labels_batch == label).nonzero(as_tuple=True)[0]\n",
    "                neg_indices = (labels_batch != label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if len(pos_indices) <= 1 or len(neg_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                while pos_idx == i:\n",
    "                    pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,)).item()]\n",
    "\n",
    "                anchor_embs.append(attention_model(anchor))\n",
    "                positive_embs.append(attention_model(embeddings_batch[pos_idx]))\n",
    "                negative_embs.append(attention_model(embeddings_batch[neg_idx]))\n",
    "\n",
    "            if anchor_embs:\n",
    "                anchor_embs = torch.stack(anchor_embs)\n",
    "                positive_embs = torch.stack(positive_embs)\n",
    "                negative_embs = torch.stack(negative_embs)\n",
    "\n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader)\n",
    "        avg_eval_loss = evaluate_attention(attention_model, eval_df, embedding_cols, eval_labels,\n",
    "                                           batch_size=batch_size, margin=margin)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.4f} | Eval Loss = {avg_eval_loss:.4f}\")\n",
    "\n",
    "    return attention_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4513a6-77de-44f8-8fb9-0d7d26b988fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attention(model, df, embedding_cols, labels, batch_size=32, margin=1.0):\n",
    "    model.eval()\n",
    "    criterion = nn.TripletMarginLoss(margin=margin)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    dataset = EmbeddingDataset(df, embedding_cols, encoded_labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            embeddings_batch, labels_batch = batch\n",
    "            anchor_embs, positive_embs, negative_embs = [], [], []\n",
    "\n",
    "            for i in range(len(labels_batch)):\n",
    "                anchor = embeddings_batch[i]\n",
    "                label = labels_batch[i]\n",
    "\n",
    "                pos_indices = (labels_batch == label).nonzero(as_tuple=True)[0]\n",
    "                neg_indices = (labels_batch != label).nonzero(as_tuple=True)[0]\n",
    "\n",
    "                if len(pos_indices) <= 1 or len(neg_indices) == 0:\n",
    "                    continue\n",
    "\n",
    "                pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                while pos_idx == i:\n",
    "                    pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,)).item()]\n",
    "                neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,)).item()]\n",
    "\n",
    "                anchor_embs.append(model(anchor))\n",
    "                positive_embs.append(model(embeddings_batch[pos_idx]))\n",
    "                negative_embs.append(model(embeddings_batch[neg_idx]))\n",
    "\n",
    "            if anchor_embs:\n",
    "                anchor_embs = torch.stack(anchor_embs)\n",
    "                positive_embs = torch.stack(positive_embs)\n",
    "                negative_embs = torch.stack(negative_embs)\n",
    "\n",
    "                loss = criterion(anchor_embs, positive_embs, negative_embs)\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "    return total_loss / max(count, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "331022e6-8a53-4e00-97fb-4bf2f00966b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attention(df, embedding_cols, model):\n",
    "    df[\"embedding_overall\"] = df[embedding_cols].apply(\n",
    "        lambda row: model(\n",
    "            torch.stack([torch.tensor(x, dtype=torch.float32) for x in row])\n",
    "        ).detach().numpy(),\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6e0d633-d285-4391-accb-ed86a2d254b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name w601sxs/b1ade-embed. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentenceTransformer(\"w601sxs/b1ade-embed\", trust_remote_code=True, device=device)\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "df = pd.read_excel(\"../../data/tell_me_again_df_with_elements_v6.xlsx\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33ee02b-4de8-4440-b685-c45cd4ffca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [json.loads(x) for x in df.extracted_elements.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c7fcc4-12f3-4e97-b054-1a6b542fb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145efe1b0baa4c0c82d02057aad04f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs = model.encode(strings, batch_size=1, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0902d70-7ae0-4d55-a5ee-d894205c6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding_overall'] = list(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f59e29f-ad05-420e-99ed-2dc528ae4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Fold 1 NDCG: 0.9347159664990642\n",
      "Fold 1 Recall: 0.8889910130718955\n",
      "\n",
      "--- Fold 2 ---\n",
      "Fold 2 NDCG: 0.9478020215297551\n",
      "Fold 2 Recall: 0.8985977212971078\n",
      "\n",
      "--- Fold 3 ---\n",
      "Fold 3 NDCG: 0.9494989786101469\n",
      "Fold 3 Recall: 0.903908851884312\n",
      "\n",
      "--- Fold 4 ---\n",
      "Fold 4 NDCG: 0.9303899305885447\n",
      "Fold 4 Recall: 0.8780981595092026\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 NDCG: 0.9493959626782993\n",
      "Fold 5 Recall: 0.9068916155419224\n",
      "\n",
      "=== Cross-Validation Summary ===\n",
      "Average NDCG: 0.9424 ± 0.0081\n",
      "Average Recall: 0.8953 ± 0.0105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "\n",
    "set_seed(42)\n",
    "embedding_cols = [col for col in df.columns if col.startswith(\"embedding_\")]\n",
    "\n",
    "if not embedding_cols:\n",
    "    df, key_set = embed_elements(df, model)\n",
    "\n",
    "labels = df['label'].tolist()\n",
    "groups = df['label']\n",
    "n_splits = 5\n",
    "\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "ndcg_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, labels, groups=groups)):\n",
    "    print(f\"\\n--- Fold {fold+1} ---\")\n",
    "    \n",
    "    df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "    df_val = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    embedding_cols = [\"embedding_characters\", \"embedding_setting\", \"embedding_plot\", \"embedding_theme\", \"embedding_other\"]#\n",
    "\n",
    "    #attention_model = train_attention(\n",
    "    #    df_train, embedding_cols, df_train.label.tolist(),\n",
    "    #   df_val, df_val.label.tolist(),\n",
    "    #    #\n",
    "    #   epochs=100, batch_size=128, learning_rate=0.0001, margin=0.1\n",
    "    #)\n",
    "\n",
    "    #df_val = apply_attention(df_val, embedding_cols, attention_model)\n",
    "\n",
    "    ndcg = calculate_mean_ndcg_score(\n",
    "        labels=df_val.label.tolist(),\n",
    "        embeddings=df_val.embedding_overall.tolist()\n",
    "    )\n",
    "    recall = calculate_mean_recall(\n",
    "        labels=df_val.label.tolist(),\n",
    "        embeddings=df_val.embedding_overall.tolist()\n",
    "    )\n",
    "\n",
    "    ndcg_scores.append(ndcg)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    print(f\"Fold {fold+1} NDCG: {ndcg}\\nFold {fold+1} Recall: {recall}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Cross-Validation Summary ===\")\n",
    "print(f\"Average NDCG: {np.mean(ndcg_scores):.4f} ± {np.std(ndcg_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42eed48-c44b-447c-9b4f-a53e3343fb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
